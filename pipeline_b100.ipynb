{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vh_mint/Documents/B100_PROJECT_FOLDER/B100/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain_docling import DoclingLoader\n",
        "from langchain_docling.loader import ExportType\n",
        "def pass_to_markdown(file_path):\n",
        "\n",
        "    file_name = os.path.basename(file_path)\n",
        "    \n",
        "    file_name = os.path.join(\"data_md\", file_name)\n",
        "\n",
        "    loader = DoclingLoader(\n",
        "            file_path=file_path,\n",
        "            export_type=ExportType.MARKDOWN\n",
        "            )\n",
        "\n",
        "    docs = loader.load()\n",
        "\n",
        "    clean_doc = docs[0].page_content.replace(\"glyph&lt;c=3,font=/CIDFont+F8&gt;\", \"\").replace(\"glyph&lt;c=3,font=/CIDFont+F5&gt;\", \"\").replace(\"glyph<c=3,font=/CIDFont+F5>\", \" \").replace(\"glyph<c=3,font=/CIDFont+F8>\", \" \")\n",
        "\n",
        "    with open(f\"{file_name}_md.md\", \"w\", encoding=\"utf-8\") as markdown_file:\n",
        "        markdown_file.write(clean_doc)\n",
        "    return f\"{file_name}_md.md\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "def clean_docs(file_path):\n",
        "\n",
        "    def process_row(line):\n",
        "        cells = [cell.strip() for cell in line.split('|')[1:-1]]\n",
        "        return '| ' + ' | '.join(cells) + ' |'\n",
        "    \n",
        "    # Carregar o JSON com as correções\n",
        "    with open('miss_words.json', 'r', encoding='utf-8') as f:\n",
        "        correcoes = json.load(f)[\"replacements\"]\n",
        "\n",
        "    # Ler o documento original\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        texto = f.readlines()\n",
        "\n",
        "    texto_corrigido = ''\n",
        "\n",
        "    in_table = False\n",
        "\n",
        "    for line in texto:\n",
        "        for erro, correcao in correcoes.items():\n",
        "            line = line.replace(erro, correcao)\n",
        "\n",
        "        stripped_line = line.strip()\n",
        "\n",
        "\n",
        "        if stripped_line.startswith('|') and stripped_line.endswith('|'):\n",
        "            if not in_table:\n",
        "                in_table = True\n",
        "            processed_line = process_row(stripped_line)\n",
        "            texto_corrigido += processed_line + \"\\n\"\n",
        "        else:\n",
        "            texto_corrigido += stripped_line + \"\\n\"\n",
        "            if in_table:\n",
        "                in_table = False\n",
        "        \n",
        "\n",
        "    file_name = os.path.basename(file_path)\n",
        "    \n",
        "    file_name = os.path.join(\"data_md_correct\", file_name)\n",
        "\n",
        "    # Salvar o documento corrigido\n",
        "    with open(f\"{file_name}_correct.md\", 'w', encoding='utf-8') as f:\n",
        "        f.write(texto_corrigido)\n",
        "    return file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14FrvCtZOtGu",
        "outputId": "9dd6497b-99ed-48b7-a3e3-a4bb611dbe45"
      },
      "outputs": [],
      "source": [
        "def extract_markdown_tables(file_path):\n",
        "    def process_row(line):\n",
        "        cells = [cell.strip() for cell in line.split('|')[1:-1]]  \n",
        "        return '| ' + ' | '.join(cells) + ' |'  # Reconstroi a linha com 1 espaço entre texto e \"|\"\n",
        "    \n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    \n",
        "    tables = []\n",
        "    current_table = []\n",
        "    in_table = False\n",
        "    \n",
        "    for line in lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Verifica se é uma linha de tabela\n",
        "        if stripped_line.startswith('|') and stripped_line.endswith('|'):\n",
        "            if not in_table:\n",
        "                in_table = True\n",
        "            processed_line = process_row(stripped_line)\n",
        "            current_table.append(processed_line)\n",
        "        else:\n",
        "            if in_table:\n",
        "                if current_table:\n",
        "                    tables.append(current_table)\n",
        "                    current_table = []\n",
        "                in_table = False\n",
        "    \n",
        "    # Adiciona a última tabela se o arquivo terminar com uma tabela\n",
        "    if in_table and current_table:\n",
        "        tables.append(current_table)\n",
        "    \n",
        "    return tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/citros.pdf\n",
            "data_md/citros.pdf_md.md\n",
            "data_md_correct/citros.pdf_md.md\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "for i, file in enumerate(os.listdir('data')):\n",
        "    \n",
        "    start_file_path = 'data/'+file\n",
        "    print(start_file_path)\n",
        "\n",
        "    markdown_path = pass_to_markdown(start_file_path)\n",
        "    print(markdown_path)\n",
        "\n",
        "    end_file_path = clean_docs(markdown_path)\n",
        "    print(end_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n",
            "168\n",
            "906\n",
            "897\n",
            "581\n",
            "3164\n",
            "1427\n",
            "4824\n",
            "4832\n",
            "5206\n",
            "4790\n",
            "1692\n"
          ]
        }
      ],
      "source": [
        "#Chunking\n",
        "from typing import List, Tuple, Optional\n",
        "import re\n",
        "\n",
        "class MarkdownHeaderRecursiveSplitter:\n",
        "    def __init__(\n",
        "        self,\n",
        "        chunk_size: int = 5000,\n",
        "        chunk_overlap: int = 400,\n",
        "        separators: List[str] = None,\n",
        "        header_pattern: str = r\"^#{1,6}\\s.+\"\n",
        "    ):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.header_regex = re.compile(header_pattern, re.MULTILINE)\n",
        "        self.separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        \n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Splits markdown text into chunks with header context\"\"\"\n",
        "        header_sections = self._split_by_headers(text)\n",
        "        chunks = []\n",
        "        \n",
        "        for header, content in header_sections:\n",
        "            if not content:\n",
        "                continue\n",
        "                \n",
        "            \n",
        "            section_chunks = self._recursive_split(\n",
        "                text=content,\n",
        "                separators=self.separators,\n",
        "                chunk_size=self.chunk_size,\n",
        "                chunk_overlap=self.chunk_overlap\n",
        "            )\n",
        "            \n",
        "            \n",
        "            for chunk in section_chunks:\n",
        "                if header:\n",
        "                    chunks.append(f\"{header}\\n{chunk}\")\n",
        "                else:\n",
        "                    chunks.append(chunk)\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "    def _split_by_headers(self, text: str) -> List[Tuple[Optional[str], str]]:\n",
        "        \"\"\"Splits text into (header, content) sections\"\"\"\n",
        "        lines = text.split('\\n')\n",
        "        sections = []\n",
        "        current_header = None\n",
        "        current_content = []\n",
        "\n",
        "        for line in lines:\n",
        "            if self.header_regex.match(line):\n",
        "                if current_header or current_content:\n",
        "                    sections.append((current_header, '\\n'.join(current_content)))\n",
        "                    current_content = []\n",
        "                current_header = line\n",
        "            else:\n",
        "                current_content.append(line)\n",
        "                \n",
        "        if current_header or current_content:\n",
        "            sections.append((current_header, '\\n'.join(current_content)))\n",
        "            \n",
        "        return sections\n",
        "\n",
        "    def _recursive_split(\n",
        "        self,\n",
        "        text: str,\n",
        "        separators: List[str],\n",
        "        chunk_size: int,\n",
        "        chunk_overlap: int\n",
        "    ) -> List[str]:\n",
        "        \"\"\"Recursively splits text using separators\"\"\"\n",
        "        chunks = []\n",
        "        remaining = text\n",
        "        \n",
        "        while len(remaining) > chunk_size:\n",
        "            split_pos = -1\n",
        "            for sep in separators:\n",
        "                pos = remaining.find(sep, chunk_size - chunk_overlap)\n",
        "                if pos != -1 and pos < chunk_size + chunk_overlap:\n",
        "                    split_pos = pos + len(sep)\n",
        "                    break\n",
        "                    \n",
        "            if split_pos == -1:\n",
        "                split_pos = chunk_size\n",
        "                \n",
        "            chunk = remaining[:split_pos]\n",
        "            chunks.append(chunk)\n",
        "            remaining = remaining[split_pos - chunk_overlap:]\n",
        "            \n",
        "        if remaining:\n",
        "            chunks.append(remaining)\n",
        "            \n",
        "        return chunks\n",
        "\n",
        "\n",
        "splitter = MarkdownHeaderRecursiveSplitter(separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
        "with open('data_md_correct/citros.pdf_md.md_correct.md', 'r') as f:\n",
        "    text = f.read()\n",
        "    chunks = splitter.split_text(text)\n",
        "\n",
        "print(len(chunks))\n",
        "for chunk in chunks:\n",
        "    print(len(chunk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load data in Qdrant\n",
        "\n",
        "from qdrant_client import QdrantClient, models\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import Literal, List, Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# client = QdrantClient(url=os.getenv(\"QDRANT_URL\"), api_key=os.getenv(\"QDRANT_API_KEY\"))\n",
        "\n",
        "client = QdrantClient(url=\"http://localhost:6333\")\n",
        "\n",
        "client\n",
        "\n",
        "class QdrantCollection(BaseModel):\n",
        "    client: QdrantClient\n",
        "    name: str\n",
        "    model_name: str\n",
        "    distance: Literal[\"Cosine\", \"Euclid\", \"Dot\"]\n",
        "    model: SentenceTransformer = None  \n",
        "\n",
        "    def __init__(self, **data):\n",
        "        super().__init__(**data)\n",
        "        # Initialize the model and store it as an instance attribute\n",
        "        self.model = SentenceTransformer(self.model_name)\n",
        "\n",
        "\n",
        "    def create(self):\n",
        "      if not self.client.collection_exists(collection_name=self.name):\n",
        "          self.client.create_collection(\n",
        "              collection_name=self.name,\n",
        "              vectors_config=models.VectorParams(\n",
        "                  size=self.model.get_sentence_embedding_dimension(),\n",
        "                  distance=self.distance\n",
        "              )\n",
        "          )\n",
        "          print(\"Collection created\")\n",
        "      else:\n",
        "          print(\"Collection already exists\")\n",
        "\n",
        "    def add_points(self, docs, metadata):\n",
        "      embeddings = self.model.encode(docs)\n",
        "\n",
        "      self.client.upsert(\n",
        "          collection_name=self.name,\n",
        "          points=[\n",
        "        models.PointStruct(\n",
        "            id=client.count(collection_name=self.name).count+idx,\n",
        "            vector=emb,\n",
        "            payload=metadata[idx]\n",
        "        )\n",
        "          for idx, emb in enumerate(embeddings)\n",
        "          ]\n",
        "      )\n",
        "      print(\"Data added\")\n",
        "\n",
        "    def delete_collection(self):\n",
        "      self.client.delete_collection(collection_name=self.name)\n",
        "      print(\"Collection deleted\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True # Allow arbitrary types like QdrantClient\n",
        "\n",
        "collections = QdrantCollection(client=client, name=\"test_splitter\", model_name=\"BAAI/bge-m3\", distance=\"Cosine\")\n",
        "collections.delete_collection()\n",
        "collections.create()\n",
        "\n",
        "for idx, chunks in enumerate(chunks):\n",
        "    print(\"=\"*200)\n",
        "    collections.add_points(chunks, metadata=[{\"content\": f\"{chunk}\"} for chunk in chunks])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
